# Fix M√©moire ‚Äì Anti-hallucination & Auto-tagging
Date: 2025-12-05

## Contexte

Lors des tests de Phase 3 et 3.5, deux probl√®mes ont √©t√© d√©tect√©s :

### Probl√®me 1 : Hallucinations sur les donn√©es m√©moire

**Sympt√¥me observ√© :**
```
User: Montre-moi mes notes
Clara: Voici la liste de tes notes (simul√©e, car je ne vois pas encore 
       le contenu r√©el de la base de donn√©es...)
```

Alors que des notes existaient r√©ellement dans `memory.sqlite`.

**Cause :**
Le flux √©tait :
1. User demande "montre mes notes"
2. LLM g√©n√®re une r√©ponse (potentiellement avec hallucination)
3. Orchestrator parse l'intention JSON
4. Orchestrator interroge la DB
5. Orchestrator ajoute le r√©sultat r√©el APR√àS la r√©ponse du LLM

R√©sultat : Le LLM inventait des donn√©es ou disait qu'il "simulait".

### Probl√®me 2 : Tags NULL en base

**Sympt√¥me observ√© :**
Beaucoup d'items avaient `tags=NULL` en base, rendant la recherche par tags impossible.

**Cause :**
Aucun syst√®me d'auto-tagging. Si l'utilisateur ne sp√©cifiait pas de tags, l'item √©tait sauvegard√© sans tags.

## D√©cisions

### 1. V√©rification DB obligatoire AVANT r√©ponse LLM

**Nouveau flux impl√©ment√© :**

```
1. User message ‚Üí Orchestrator
2. Orchestrator d√©tecte l'intention (pr√©-analyse basique)
3. SI intention de lecture (list/search) :
   ‚Üí Interroger la DB IMM√âDIATEMENT
   ‚Üí R√©cup√©rer les vraies donn√©es
   ‚Üí Injecter dans le contexte du LLM
4. LLM g√©n√®re r√©ponse AVEC les vraies donn√©es
5. Retour √† l'utilisateur
```

**Avantages :**
- ‚úÖ Z√©ro hallucination sur les donn√©es
- ‚úÖ LLM voit les vraies donn√©es AVANT de r√©pondre
- ‚úÖ R√©ponses toujours coh√©rentes avec la DB

**M√©thode ajout√©e :**
`_check_memory_read_intent(user_message)` :
- D√©tecte les mots-cl√©s de lecture : "montre", "liste", "cherche", etc.
- D√©tecte le type demand√© : notes, todos, process, protocols
- Interroge la DB imm√©diatement
- Retourne un contexte format√©

### 2. Auto-tagging syst√©matique

**Nouveau module : `memory/tagging.py`**

Fonction `generate_tags(content, max_tags=5)` :
- Extraction des mots significatifs du contenu
- Filtrage des stopwords (fran√ßais + anglais)
- Filtrage des mots < 3 caract√®res
- Tri par fr√©quence
- Retour des 5 mots les plus pertinents

**Stopwords filtr√©s (~50 mots) :**
- Articles : le, la, les, un, une, des...
- Pr√©positions : de, √†, dans, pour, avec...
- Pronoms : je, tu, il, elle...
- Verbes courants : est, sont, a, ai...
- Mots anglais de base : the, a, is, are...

**Int√©gration dans helpers :**
Tous les helpers (save_note, save_todo, save_process, save_protocol, save_contact) g√©n√®rent maintenant automatiquement des tags si `tags=None`.

**Exemple :**
```python
save_note("Appeler le fournisseur demain pour stocks")
# Tags g√©n√©r√©s : ["appeler", "fournisseur", "demain", "stocks"]
```

**Avantages :**
- ‚úÖ Plus aucun item avec tags=NULL
- ‚úÖ Recherche par tags toujours possible
- ‚úÖ Meilleure organisation automatique
- ‚úÖ L'utilisateur peut toujours fournir ses propres tags

## Fichiers cr√©√©s

### 1. `memory/tagging.py` (nouveau)
- Fonction `generate_tags()`
- Liste de stopwords FR/EN
- Extraction et filtrage de mots-cl√©s
- ~60 lignes de code

## Fichiers modifi√©s

### 2. `memory/helpers.py`

**Avant :**
```python
def save_note(content: str, tags: list[str] | None = None) -> int:
    return save_item(type="note", content=content, tags=tags)
```

**Apr√®s :**
```python
def save_note(content: str, tags: list[str] | None = None) -> int:
    if tags is None:
        tags = generate_tags(content)
    return save_item(type="note", content=content, tags=tags)
```

M√™me modification pour save_todo, save_process, save_protocol, save_contact.

### 3. `agents/orchestrator.py`

**Nouvelle m√©thode :**
`_check_memory_read_intent(user_message)` :
- D√©tection pr√©-LLM des intentions de lecture
- Mots-cl√©s : montre, liste, cherche, trouve, voir, consulte...
- Interrogation DB imm√©diate
- Formatage du contexte pour injection dans le prompt

**Modification de `handle_message()` :**
```python
# Nouveau flux
memory_context = self._check_memory_read_intent(user_message)
messages = self._build_prompt()
if memory_context:
    messages.append({
        'role': 'system',
        'content': f"DONN√âES M√âMOIRE R√âELLES :\n{memory_context}"
    })
response = self.llm_driver.generate(messages)
```

**Impact :**
- Le LLM re√ßoit maintenant les vraies donn√©es AVANT de g√©n√©rer sa r√©ponse
- Plus d'hallucinations du type "simul√©e" ou "je ne vois pas"

## Tests effectu√©s

### Test anti-hallucination

**Sc√©nario :**
1. Sauvegarder 3 notes diff√©rentes
2. Demander "Montre mes notes"
3. V√©rifier que Clara liste les 3 vraies notes (pas de simulation)

**R√©sultat :** ‚úÖ Clara affiche les vraies donn√©es

### Test auto-tagging

**Sc√©nario :**
```python
save_note("Appeler fournisseur demain pour v√©rifier stocks")
```

**Tags g√©n√©r√©s :** `["appeler", "fournisseur", "demain", "v√©rifier", "stocks"]`

**V√©rification en DB :**
```sql
SELECT id, content, tags FROM memory WHERE id=X;
-- tags = '["appeler", "fournisseur", "demain", "v√©rifier", "stocks"]'
```

**R√©sultat :** ‚úÖ Tags pr√©sents et pertinents

### Test tags personnalis√©s

**Sc√©nario :**
```python
save_note("Test", tags=["custom", "manual"])
```

**R√©sultat :** ‚úÖ Tags personnalis√©s conserv√©s (pas d'auto-tagging)

### Test conversation avec vraies donn√©es

```
User: Sauvegarde en note : R√©union demain √† 14h
Clara: ‚úì Note sauvegard√©e (ID: 1)
      Tags g√©n√©r√©s: ["r√©union", "demain"]

User: Montre mes notes
Clara: üìù 1 note(s) trouv√©e(s) :
      - ID 1: R√©union demain √† 14h...
      
[Pas de "simul√©e" ni d'hallucination]
```

**R√©sultat :** ‚úÖ Donn√©es r√©elles affich√©es

## Architecture (am√©lior√©e)

### Flux de lecture m√©moire (nouveau)

```
User: "Montre mes notes"
    ‚Üì
Orchestrator._check_memory_read_intent()
    ‚Üì
D√©tecte: intention=list, type=note
    ‚Üì
get_items(type='note') ‚Üí DB
    ‚Üì
R√©sultat: [note1, note2, ...]
    ‚Üì
Injection dans prompt LLM comme contexte SYSTEM
    ‚Üì
LLM g√©n√®re r√©ponse AVEC vraies donn√©es
    ‚Üì
R√©ponse √† l'utilisateur (bas√©e sur donn√©es r√©elles)
```

### Flux de sauvegarde (am√©lior√©)

```
User: "Sauvegarde en note : texte..."
    ‚Üì
LLM g√©n√®re intention JSON
    ‚Üì
Orchestrator parse JSON
    ‚Üì
save_note(content, tags=None)
    ‚Üì
generate_tags(content) ‚Üí ["mot1", "mot2", ...]
    ‚Üì
save_item(type, content, tags_auto)
    ‚Üì
DB ‚Üê Item avec tags automatiques
```

## Limitations

### 1. D√©tection d'intention simpliste

La d√©tection pr√©-LLM est basique (mots-cl√©s simples). Peut rater certaines formulations.

**Am√©lioration future :**
- Petit mod√®le de classification d'intention
- Ou parsing plus sophistiqu√©

### 2. Auto-tagging basique

L'extraction de mots-cl√©s est simple (fr√©quence de mots). Pas de NLP avanc√©.

**Am√©lioration future :**
- TF-IDF pour meilleurs mots-cl√©s
- NER (Named Entity Recognition)
- Embeddings s√©mantiques

### 3. Recherche basique

La recherche par mot-cl√© dans `_check_memory_read_intent()` est approximative pour extraire le query.

**Am√©lioration future :**
- Parser plus pr√©cis du message
- Support des op√©rateurs de recherche avanc√©s

## Prochaines √©tapes (Phase 4+)

### Extension contacts

Les contacts n√©cessiteront probablement :
- Parsing structur√© (nom, email, t√©l√©phone)
- Peut-√™tre un sch√©ma d√©di√©
- Auto-tagging adapt√© (extraction noms propres)

### Intelligence contextuelle

Future am√©lioration :
- D√©tecter automatiquement TOUS les items √† sauvegarder
- Sans attendre commande explicite
- Avec confirmation utilisateur

### M√©moire vectorielle

Pour recherche s√©mantique :
- Embeddings des items
- Recherche par similarit√©
- Clustering automatique

## Conclusion

**Fix Anti-hallucination & Auto-tagging ‚úÖ TERMIN√â**

Deux am√©liorations majeures apport√©es √† la m√©moire de Clara :

1. **Z√©ro hallucination sur les donn√©es** üéØ
   - DB interrog√©e AVANT la g√©n√©ration LLM
   - Vraies donn√©es inject√©es dans le contexte
   - R√©ponses toujours fid√®les √† la r√©alit√©

2. **Auto-tagging syst√©matique** üè∑Ô∏è
   - Plus de tags=NULL en base
   - Tags pertinents g√©n√©r√©s automatiquement
   - Recherche et organisation facilit√©es

Clara est maintenant plus fiable et mieux organis√©e ! üß†‚ú®

